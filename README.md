# Linear Regression from Scratch
This project implements simple and multiple linear regression from scratch using **Ordinary Least Squares (OLS)**. The accompanying Jupyter notebook contains an overview of the equations used to represent simple and multiple regression, the objective function being minimised, the computations that follow from OLS, and calculation of $R^2$ to assess model performance.

## Technical Implementation
This project was implemented using Jupyter Notebook, version 4.20.0. <br>
**Libraries used:** NumPy, Matplotlib. <br>
NumPy was used for all computations, and Matplotlib was used for accompanying visualisation.
Additionally, sample datasets of 50 observations were created by applying random error to arrays generated by NumPy functions. The arrays without added error acted as the datasets containing the truth labels.

## Mathematical Implementation
This project contains a more thorough explanation of the equations used to express both simple and multiple regression, as well as the parameters being estimated in each case. Briefly, OLS minimises the objective function, the Sum of Squared Errors (SSE), by setting the derivative of said function with respect to the parameters to zero. This process determines the best set of parameter values that minimises the objective function and thus maximises the model's prediction accuracy.

## Results and Insights
The difference in the $R^2$ values of simple and multiple regression arises from differences in the datasets used in each case. The simple regression dataset exhibits a strong linear relationship, likely due to a limited range for added error. On the other hand, the multiple regression dataset contains more noise and unobserved variability due to a significantly larger range of added error, leading to a lower but realistic $R^2$.

## Limitations and Improvement
This project does not implement multiple linear regression with regularisation. Therefore, the model may not be as robust as others that use Lasso (L1) or Ridge (L2) regression to remove irrelevant features or reduce their weight in predictions. Additionally, a trainâ€“test split could be introduced to study how the model generalises, but it was not necessary as that was outside the scope of this implementation-focused project.
